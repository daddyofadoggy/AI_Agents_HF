{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQq1kELL-Rr9"
   },
   "source": [
    "# Tools in LlamaIndex\n",
    "\n",
    "\n",
    "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
    "\n",
    "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
    "\n",
    "## Let's install the dependencies\n",
    "\n",
    "We will install the dependencies for this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rQ6BS3KM-Rr-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/ron/Documents/github/myenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WCOWxcO-Rr_"
   },
   "source": [
    "And, let's log in to Hugging Face to use serverless Inference APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "84iBbHgz-Rr_"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f9dc561956469eb33aadc702abac3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBAKvXyK-Rr_"
   },
   "source": [
    "## Creating a FunctionTool\n",
    "\n",
    "Let's create a basic `FunctionTool` and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wh83uY7E-Rr_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in New York is sunny')], tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5Ew31Zf-Rr_"
   },
   "source": [
    "## Creating a QueryEngineTool\n",
    "\n",
    "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W3yV9r5B-Rr_",
    "outputId": "aa033b37-23c8-471a-8d7e-b5be87f5c1e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='This individual is likely to be interested in research that explores the intersection of technology and environmental conservation, potentially examining how AI can be used to support sustainable practices and mitigate the effects of climate change on ecosystems.')], tool_name='some useful name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response='This individual is likely to be interested in research that explores the intersection of technology and environmental conservation, potentially examining how AI can be used to support sustainable practices and mitigate the effects of climate change on ecosystems.', source_nodes=[NodeWithScore(node=TextNode(id_='1233fd19-4531-432d-bd12-4f95a4f21198', embedding=None, metadata={'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1002.txt', 'file_name': 'persona_1002.txt', 'file_type': 'text/plain', 'file_size': 122, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7a7e282d-064c-4ae3-8f78-f125d4b1bb89', node_type='4', metadata={'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1002.txt', 'file_name': 'persona_1002.txt', 'file_type': 'text/plain', 'file_size': 122, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}, hash='e426b4dd7292493e17912bd7033effdd404e91fd3e6dccf34e236db8bf452715')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An ecologist specializing in climate change impacts on regional ecosystems, particularly those with diverse plant species.', mimetype='text/plain', start_char_idx=0, end_char_idx=122, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4111877431457492), NodeWithScore(node=TextNode(id_='1d5452f1-7f4a-4a6a-bd03-035814641e5d', embedding=None, metadata={'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1004.txt', 'file_name': 'persona_1004.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6ffa91cb-f926-433a-b46c-49b079c1f92a', node_type='4', metadata={'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1004.txt', 'file_name': 'persona_1004.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}, hash='acb2fc9b761493bfbb2b3f138a23e5e48fdfe345d67016f21341e6d7ad8369e1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An environmental historian or urban planner focused on ecological conservation and sustainability, likely working in local government or a related organization.', mimetype='text/plain', start_char_idx=0, end_char_idx=160, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4037788994976229)], metadata={'1233fd19-4531-432d-bd12-4f95a4f21198': {'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1002.txt', 'file_name': 'persona_1002.txt', 'file_type': 'text/plain', 'file_size': 122, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}, '1d5452f1-7f4a-4a6a-bd03-035814641e5d': {'file_path': '/Users/ron/Documents/github/AI_Agents_HF/Llamaindex/data/persona_1004.txt', 'file_name': 'persona_1004.txt', 'file_type': 'text/plain', 'file_size': 160, 'creation_date': '2025-07-20', 'last_modified_date': '2025-07-20'}}), is_error=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Llama-3.2-3B-Instruct\", provider=\"auto\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"some useful name\",\n",
    "    description=\"some useful description\",\n",
    ")\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXiU5Dn8-Rr_"
   },
   "source": [
    "## Creating Toolspecs\n",
    "\n",
    "Let's create a `ToolSpec` from the `GmailToolSpec` from the LlamaHub and convert it to a list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-gd54_f9-RsA",
    "outputId": "6bd971e7-b4e5-4e2a-86c7-0c9433c11b88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x1077e2880>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x360835490>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x11c304f70>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x11c304040>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3608935b0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3611f4040>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'load_data'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_spec_list[0].metadata.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_spec_list[0].metadata.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcfRcnZF-RsA"
   },
   "source": [
    "To get a more detailed view of the tools, we can take a look at the `metadata` of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYid5S9x-RsA",
    "outputId": "22f09c21-3f07-4bc9-fbf9-411e46bb329a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "search_messages search_messages(query: str, max_results: Optional[int] = None)\n",
      "Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query[str]: The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "        \n",
      "create_draft create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "        \n",
      "update_draft update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "get_draft get_draft(draft_id: str = None) -> str\n",
      "Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "send_draft send_draft(draft_id: str = None) -> str\n",
      "Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
